{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b122991-860d-4761-bdbc-5a5337b0f968",
   "metadata": {},
   "source": [
    "**PART A: Build a baseline model**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a323bd-dc94-4ffc-bd52-aa1044e0893f",
   "metadata": {},
   "source": [
    "Use the Keras library to build a neural network with the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9edb89b-211f-440f-972f-ce0ba17d12ac",
   "metadata": {},
   "source": [
    "- One hidden layer of 10 nodes, and a ReLU activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61326b88-bd89-40a1-8391-91c732d33007",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3168d293-2403-4269-a461-573cb9d01b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "\n",
    "data = pd.read_csv('https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0101EN/labs/data/concrete_data.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42750673-bf94-4090-bfde-93d46f84ba33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check size of the dataset\n",
    "\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3bf3cc-5e11-4aa6-8659-6076a62ed74a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarize the dataset\n",
    "\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a113e908-a2e0-4fa6-bccd-797255eafa9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Are there missing values?\n",
    "\n",
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "586ac1c4-2cc9-4510-af3c-b018ae799429",
   "metadata": {},
   "source": [
    "These clean data can be used to build model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af8aa5e-ef5d-47bf-b05e-482452f96232",
   "metadata": {},
   "source": [
    "Split data into predictors and target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8828d84-fe00-4990-87c8-8b744a50cf60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"Strength\" is our target column\n",
    "# All other columns are our predictors columns\n",
    "\n",
    "data_columns = data.columns\n",
    "\n",
    "predictors = data[data_columns[data_columns != 'Strength']]\n",
    "print(target.head())\n",
    "\n",
    "target = data['Strength']\n",
    "print(predictors.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3228d126-301d-464e-b619-ad90d960545c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of predictors\n",
    "\n",
    "n_cols = predictors_norm.shape[1]\n",
    "n_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e333bbc-5ae3-42f9-a76f-58f10f726943",
   "metadata": {},
   "source": [
    "**Building the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "243e3b6a-fa62-4121-877e-3614c3b57e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "\n",
    "# Other functions\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f481407-aee2-4ccf-89c6-dae02f3eeaad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define NN\n",
    "\n",
    "# - One hidden layer of 10 nodes, and a ReLU activation function\n",
    "# - Use the adam optimizer and the mean squared error  as the loss function.\n",
    "\n",
    "def regression_model():\n",
    "    #create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(10, activation='relu', input_shape=(n_cols,)))\n",
    "    model.add(Dense(1))\n",
    "    \n",
    "    # compile model\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb45da1-88e1-4a40-85e3-f9461bc2ff8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset to train_set and test_set\n",
    "# Randomly split the data into a training and test sets by holding 30% of the data for testing.\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(predictors, target, test_size=0.3, random_state=1)\n",
    "print ('Train set:', X_train.shape,  y_train.shape)\n",
    "print ('Test set:', X_test.shape,  y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e19d55-eb34-457f-a973-cd80a7af6e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model\n",
    "model_A = regression_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3669a34-f28b-4a1f-b339-1004acc9fbf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a model\n",
    "# Train the model on the training data using 50 epochs.\n",
    "\n",
    "model_A.fit(X_train, y_train, epochs=50, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5ae0bf-8f92-4457-a77c-7b642bc61078",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "loss_val = model_A.evaluate(X_test, y_test)\n",
    "y_pred = model_A.predict(X_test)\n",
    "loss_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b5737d-9b23-4f42-a4ba-b124059d186f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test data\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "mean_square_error = mean_squared_error(y_test, y_pred)\n",
    "mean = np.mean(mean_square_error)\n",
    "standard_deviation = np.std(mean_square_error)\n",
    "print(mean, standard_deviation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d214906-4ef6-4124-85cf-5e356206e423",
   "metadata": {},
   "source": [
    "Repeat the previous steps and create a list of **50** mean squared errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ed02dc-7fe2-4c7a-bc41-23a58856e379",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 50\n",
    "mean_squared_errors = []\n",
    "for i in range(0, 50):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(predictors, target, test_size=0.3, random_state=i)\n",
    "    model_A.fit(X_train, y_train, epochs=epochs, verbose=0)\n",
    "    MSE = model_A.evaluate(X_test, y_test, verbose=0)\n",
    "    print(\"Loop -  \"+str(i+1)+\": \"+str(MSE))\n",
    "    y_pred = model_A.predict(X_test)\n",
    "    mean_square_error = mean_squared_error(y_test, y_pred)\n",
    "    mean_squared_errors.append(mean_square_error)\n",
    "\n",
    "mean_squared_errors = np.array(mean_squared_errors)\n",
    "mean = np.mean(mean_squared_errors)\n",
    "standard_deviation = np.std(mean_squared_errors)\n",
    "\n",
    "print('\\n')\n",
    "print(\"Mean: \"+str(mean))\n",
    "print(\"Standard Deviation: \"+str(standard_deviation))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "143226d87d878038d06aa813892203fb36ee940f643061de2c8867458f553e7b"
  },
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
